{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 315 Project Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marisa Papagelis and Natalie Reid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will collect data for our project. At the most basic level, we are scraping Facebook pages for post content (text, likes, and comments). Additional information will be added to this section as our project progresses.\n",
    "\n",
    "**All data was scraped from Reuters, MSNBC, and FoxNews Facebook pages on October 6, 2020 between 11am and 3pm EDT.** \n",
    "\n",
    "We will compliment this notebook with Exploring_Data_CS315_Project_MarisaPapagelis_NatalieReid, a notebook for our data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Initial Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data collection, we modified code created by Junita Sirait in order to scrape Facebook news source pages. We have separated our data collection into three parts, one for each news source, below. We used Selenium and a few other Python libraries (BeautifulSoup, Pandas, etc.). We used selenium to automate scrolling through our Facebook pages and BeautifulSoup to scrape each page to collect our data. \n",
    "\n",
    "We have noted the modifications we made to Juita's code as we run through it, but it was helpful to use her code as a starting point as she created it as a teaching opportunity for our CS315 class, and her process aligned with our goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Scraping Reuters Facebook page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 1, we scrape the Reuters Facebook page. We will complete this process two additional times after our initial one for our additional two news sources. The process will be idential with only a change to the scroll time of selenium accounting for post length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we needed to use chromedriver to open a chrome browser, and then a Facebook page, that we could manipulate and scroll through using selenium. After reaching Facebook on our Chrome browser, we manually logged into Facebook and navigated to Reuters Facebook page. We understand that this could be done through our Python code, but for simplicity in our trial exploration, we navigated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='Downloads/chromedriver') # this will open a new page of Google Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://facebook.com\") #manually logged into FB and navigated to Reuters FB page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we used selenium to scroll down Reuters Facebook page and collect posts. \n",
    "We modified the scrolling time to give selenium enough time to collect our ideal amount of posts (~750)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 300: \n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            i+=1\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we saved the file as an HTML page to be used for future parsing. We note that the encoding parameter is important because the HTML page contains characters that are encoded as Unicode (e.g. emoji-s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Reuters.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use BeautifulSoup along with some modified helper functions to parce through the data from our saved HTML file. We inspected our Facebook page in our browser to find the class names for the post text, the likes, and the comments. We then used BeautifulSoup's .find function to pull the appropriate content from each post. We further modified Junita's original getLikesCommentsShares() function into our own getLikesComments() function to pull only the information that we needed (likes and comments). We also split the comments into a list of two strings for easier manipulation in the future, and we got rid of the links because we didn't need them for our exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup= BeautifulSoup(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two helper functions that we modified, as described above, are below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostText(content): \n",
    "    \"\"\"A helper function to extrac the text of each post.\n",
    "    It's possible that class names might change over time.\n",
    "    \"\"\"\n",
    "    post_el = content.find(\"div\", \n",
    "                           class_=\"_5pbx userContent _3576\")\n",
    "    if post_el: \n",
    "        return post_el.text\n",
    "    else: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLikesComments(article):\n",
    "    \"\"\"\n",
    "    A helper file to extract the statistics of engagement for each post.\n",
    "    \"\"\"\n",
    "    likes = 0 \n",
    "    comments = 0\n",
    "    \n",
    "    # gather likes \n",
    "    likes_el = article.find(\"span\", class_=\"_81hb\")\n",
    "    if likes_el: \n",
    "        likes = likes_el.text \n",
    "        \n",
    "    # gather comments\n",
    "    comments_el = article.find(\"a\", class_=\"_3hg- _42ft\")\n",
    "    if comments_el:\n",
    "        comments = comments_el.text\n",
    "            \n",
    "    return [likes, comments.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the information from each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we inspected our Reuters Facebook page to find the class of the div containing an entire post, so we could get information from each post on the page (rather than a singular post). We took some of this code from Junita's example, but we modified it to only collect post text, likes, and comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = []\n",
    "for article in soup.find_all(\"div\", class_=\"_5pcr userContentWrapper\"):\n",
    "    try:\n",
    "        post_text = getPostText(article)\n",
    "        likes, comments = getLikesComments(article)\n",
    "        post_data.append((post_text, likes, comments))\n",
    "    except AttributeError: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save our results as a data frame so it can be used later in exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Southwest Airline &lt;LUV.N&gt; can avoid furloughs ...</td>\n",
       "      <td>6</td>\n",
       "      <td>[2, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The U.S. Securities and Exchange Commission (S...</td>\n",
       "      <td>34</td>\n",
       "      <td>[3, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The U.S. Senate Judiciary Committee announced ...</td>\n",
       "      <td>576</td>\n",
       "      <td>[106, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. commercial bankruptcy filings are up 33% ...</td>\n",
       "      <td>152</td>\n",
       "      <td>[54, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democratic presidential candidate Joe Biden ap...</td>\n",
       "      <td>1.2K</td>\n",
       "      <td>[222, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>How murder, kidnappings and miscalculation set...</td>\n",
       "      <td>83</td>\n",
       "      <td>[12, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>Australia's competition regulator said the fed...</td>\n",
       "      <td>65</td>\n",
       "      <td>[3, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Semiconductor company Mellanox Technologies ha...</td>\n",
       "      <td>58</td>\n",
       "      <td>[1, Comment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>The U.S. Senate, rushing to meet a looming dea...</td>\n",
       "      <td>95</td>\n",
       "      <td>[39, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>Alarms sound and then screens and mics are shu...</td>\n",
       "      <td>49</td>\n",
       "      <td>[9, Comments]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1091 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text likes         comments\n",
       "0     Southwest Airline <LUV.N> can avoid furloughs ...     6    [2, Comments]\n",
       "1     The U.S. Securities and Exchange Commission (S...    34    [3, Comments]\n",
       "2     The U.S. Senate Judiciary Committee announced ...   576  [106, Comments]\n",
       "3     U.S. commercial bankruptcy filings are up 33% ...   152   [54, Comments]\n",
       "4     Democratic presidential candidate Joe Biden ap...  1.2K  [222, Comments]\n",
       "...                                                 ...   ...              ...\n",
       "1086  How murder, kidnappings and miscalculation set...    83   [12, Comments]\n",
       "1087  Australia's competition regulator said the fed...    65    [3, Comments]\n",
       "1088  Semiconductor company Mellanox Technologies ha...    58     [1, Comment]\n",
       "1089  The U.S. Senate, rushing to meet a looming dea...    95   [39, Comments]\n",
       "1090  Alarms sound and then screens and mics are shu...    49    [9, Comments]\n",
       "\n",
       "[1091 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters_df # view data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into a JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save our results as a JSON file to be used later in exploration. This is so we can easily access our data in another notebook used solely for exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('reuters-posts.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view the JSON, uncomment the cell below. It is left commented for an easier to navigate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Scraping MSNBC Facebook page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 2, we will scrape MSNBC Facebook page. We will preform the same procedure as we did for Reuters, except we will adjust the scroll time to account fo MSNBC's page. To avoid redundancy, we shorten our explanations unless they are different from they were in Part 1. Refer back to Part 1 for motivation and code adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping data with Selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we open a chrome browser and manually navigate to MSNBC Facebook page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='Downloads/chromedriver') # this will open a new page of Google Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://facebook.com\") # manually logged into FB and navigated to MSNBC FB page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use selenium to scroll through the page. Since MSNBC has longer posts and a larger following, Selenium takes more time to load than Reuters did. We will increate the scroll time to account for this, so we can retrieve our ideal number of posts (~750). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 350: \n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            i+=1\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we saved the file as an HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"MSNBC.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use BeautifulSoup and some modified helper functions to extract information a post on MSNBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup= BeautifulSoup(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostText(content): \n",
    "    \"\"\"A helper function to extrac the text of each post.\n",
    "    It's possible that class names might change over time.\n",
    "    \"\"\"\n",
    "    post_el = content.find(\"div\", \n",
    "                           class_=\"_5pbx userContent _3576\")\n",
    "    if post_el: \n",
    "        return post_el.text\n",
    "    else: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLikesComments(article):\n",
    "    \"\"\"\n",
    "    A helper file to extract the statistics of engagement for each post.\n",
    "    \"\"\"\n",
    "    likes = 0 \n",
    "    comments = 0\n",
    "    \n",
    "    # gather likes \n",
    "    likes_el = article.find(\"span\", class_=\"_81hb\")\n",
    "    if likes_el: \n",
    "        likes = likes_el.text \n",
    "        \n",
    "    # gather comments\n",
    "    comments_el = article.find(\"a\", class_=\"_3hg- _42ft\")\n",
    "    if comments_el:\n",
    "        comments = comments_el.text\n",
    "            \n",
    "    return [likes, comments.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the information from each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we extract the same information (likes and comments) from each post on MSNBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = []\n",
    "for article in soup.find_all(\"div\", class_=\"_5pcr userContentWrapper\"):\n",
    "    try:\n",
    "        post_text = getPostText(article)\n",
    "        likes, comments = getLikesComments(article)\n",
    "        post_data.append((post_text, likes, comments))\n",
    "    except AttributeError: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save our results as a data frame so it can be used later in exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSNBC_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"I still don't even know how to fully characte...</td>\n",
       "      <td>299</td>\n",
       "      <td>[135, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joy Reid on President Trump removing his mask ...</td>\n",
       "      <td>1.3K</td>\n",
       "      <td>[522, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PLAN YOUR VOTE: Multiple states, such as Flori...</td>\n",
       "      <td>10</td>\n",
       "      <td>[11, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: President Trump returns to the White...</td>\n",
       "      <td>5.4K</td>\n",
       "      <td>[2.4K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WATCH: President Trump arrives at White House ...</td>\n",
       "      <td>914</td>\n",
       "      <td>[881, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>What do you know about the first ever presiden...</td>\n",
       "      <td>21</td>\n",
       "      <td>[13, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>Rep. Ocasio-Cortez stands by her use of the te...</td>\n",
       "      <td>5.3K</td>\n",
       "      <td>[2K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>LIVE: Senate confirmation hearing for UN ambas...</td>\n",
       "      <td>651</td>\n",
       "      <td>[1.5K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>BREAKING: An international team of investigato...</td>\n",
       "      <td>268</td>\n",
       "      <td>[65, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>Watch MSNBC on your AppleTV.  Download the MSN...</td>\n",
       "      <td>29</td>\n",
       "      <td>[45, Comments]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1398 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text likes  \\\n",
       "0     \"I still don't even know how to fully characte...   299   \n",
       "1     Joy Reid on President Trump removing his mask ...  1.3K   \n",
       "2     PLAN YOUR VOTE: Multiple states, such as Flori...    10   \n",
       "3     BREAKING: President Trump returns to the White...  5.4K   \n",
       "4     WATCH: President Trump arrives at White House ...   914   \n",
       "...                                                 ...   ...   \n",
       "1393  What do you know about the first ever presiden...    21   \n",
       "1394  Rep. Ocasio-Cortez stands by her use of the te...  5.3K   \n",
       "1395  LIVE: Senate confirmation hearing for UN ambas...   651   \n",
       "1396  BREAKING: An international team of investigato...   268   \n",
       "1397  Watch MSNBC on your AppleTV.  Download the MSN...    29   \n",
       "\n",
       "              comments  \n",
       "0      [135, Comments]  \n",
       "1      [522, Comments]  \n",
       "2       [11, Comments]  \n",
       "3     [2.4K, Comments]  \n",
       "4      [881, Comments]  \n",
       "...                ...  \n",
       "1393    [13, Comments]  \n",
       "1394    [2K, Comments]  \n",
       "1395  [1.5K, Comments]  \n",
       "1396    [65, Comments]  \n",
       "1397    [45, Comments]  \n",
       "\n",
       "[1398 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSNBC_df # view data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into a JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save our results as a JSON file to be used later in exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('MSNBC-posts.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view the JSON, uncomment the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSNBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scraping FOX News Facebook page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part 3, we will scrape Fox News Facebook page. We will preform the same procedure as we did for Reuters and MSNBC, except we will adjust the scroll time to account fo Fox News' page. To avoid redundancy, we shorten our explanations unless they are different from they were in Part 1. Refer back to Part 1 for motivation and code adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping data with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we open a chrome browser and manually navigate to Fox News Facebook page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='Downloads/chromedriver') # this will open a new page of Google Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://facebook.com\") #manually logged into FB and navigated to Fox News FB page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use selenium to scroll through the page. Since Fox News has longer posts and a larger following than both Reuters and MSNBC, Selenium takes more time to load than Reuters and MSNBC did. We will increate the scroll time to account for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 750: \n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            i+=1\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we saved the file as an HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"FoxNews.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use BeautifulSoup and some modified helper functions to extract information from posts on Fox News. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup= BeautifulSoup(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostText(content): \n",
    "    \"\"\"A helper function to extrac the text of each post.\n",
    "    It's possible that class names might change over time.\n",
    "    \"\"\"\n",
    "    post_el = content.find(\"div\", \n",
    "                           class_=\"_5pbx userContent _3576\")\n",
    "    if post_el: \n",
    "        return post_el.text\n",
    "    else: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLikesComments(article):\n",
    "    \"\"\"\n",
    "    A helper file to extract the statistics of engagement for each post.\n",
    "    \"\"\"\n",
    "    likes = 0 \n",
    "    comments = 0\n",
    "    shares = 0\n",
    "    \n",
    "    # gather likes \n",
    "    likes_el = article.find(\"span\", class_=\"_81hb\")\n",
    "    if likes_el: \n",
    "        likes = likes_el.text \n",
    "        \n",
    "    # gather comments\n",
    "    comments_el = article.find(\"a\", class_=\"_3hg- _42ft\")\n",
    "    if comments_el:\n",
    "        comments = comments_el.text\n",
    "            \n",
    "    return [likes, comments.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the information from each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we extract the same information (likes and comments) from each post on Fox News."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = []\n",
    "for article in soup.find_all(\"div\", class_=\"_5pcr userContentWrapper\"):\n",
    "    try:\n",
    "        post_text = getPostText(article)\n",
    "        likes, comments = getLikesComments(article)\n",
    "        post_data.append((post_text, likes, comments))\n",
    "    except AttributeError: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save our results as a data frame so it can be used later in exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "FoxNews_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JUST IN: President Donald J. Trump plans to ta...</td>\n",
       "      <td>28K</td>\n",
       "      <td>[5.1K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A new report reveals that despite some states ...</td>\n",
       "      <td>10K</td>\n",
       "      <td>[1.4K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>President Donald J. Trump arrives at the White...</td>\n",
       "      <td>62K</td>\n",
       "      <td>[17K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Donald J. Trump departs from Walter ...</td>\n",
       "      <td>30K</td>\n",
       "      <td>[6.7K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald J. Trump leaves Walter Reed M...</td>\n",
       "      <td>30K</td>\n",
       "      <td>[11K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>House Speaker Nancy Pelosi has a bill enrollme...</td>\n",
       "      <td>12K</td>\n",
       "      <td>[12K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>President Donald J. Trump pays his respects at...</td>\n",
       "      <td>127K</td>\n",
       "      <td>[49K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>President Donald J. Trump delivers remarks in ...</td>\n",
       "      <td>110K</td>\n",
       "      <td>[42K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>Oregon Governor Kate Brown holds a press confe...</td>\n",
       "      <td>2.2K</td>\n",
       "      <td>[4.6K, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>A press conference is held regarding the Spelm...</td>\n",
       "      <td>3K</td>\n",
       "      <td>[5.2K, Comments]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text likes          comments\n",
       "0    JUST IN: President Donald J. Trump plans to ta...   28K  [5.1K, Comments]\n",
       "1    A new report reveals that despite some states ...   10K  [1.4K, Comments]\n",
       "2    President Donald J. Trump arrives at the White...   62K   [17K, Comments]\n",
       "3    President Donald J. Trump departs from Walter ...   30K  [6.7K, Comments]\n",
       "4    President Donald J. Trump leaves Walter Reed M...   30K   [11K, Comments]\n",
       "..                                                 ...   ...               ...\n",
       "746  House Speaker Nancy Pelosi has a bill enrollme...   12K   [12K, Comments]\n",
       "747  President Donald J. Trump pays his respects at...  127K   [49K, Comments]\n",
       "748  President Donald J. Trump delivers remarks in ...  110K   [42K, Comments]\n",
       "749  Oregon Governor Kate Brown holds a press confe...  2.2K  [4.6K, Comments]\n",
       "750  A press conference is held regarding the Spelm...    3K  [5.2K, Comments]\n",
       "\n",
       "[751 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FoxNews_df # view data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into a JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save our results as a JSON file to be used later in exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('FoxNews-posts.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to view the JSON, uncomment the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FoxNews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Classifier Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how accurate our classifier would be on news sources that are not the three we trained it on, we chose 6 more news sources to run our classifier on. We chose two historically neutral sources, AP and Bloomberg, two left-wing, Wonkette, CNN, and two right-wing, InfoWars, The Washington Times. *Both InfoWars and Wonkette are historically less reliable and contain propagated information, so we decided to include these sources to analyze what happens.* We made these selections using an Interactive Bias Media Chart (https://www.adfontesmedia.com/interactive-media-bias-chart-2/) that we referred to when choosing our intial three sources as well as during our Literature Review.\n",
    "\n",
    "We decided to scrape 200 of the most recent posts from all six of our news sources and run them through the classifier to test to accuracy. Since our classifier was trained to work for any post, time does not need to be controlled in this step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below, we scrape 6 news sources for ~200 posts. Since the scraping process was thoroughly described in Part I, there isn't much explanation done below. We run the same bits of code on each appropriate news source. Refer to Part I for full documentation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping data with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we open a chrome browser and manually navigate to the appropriate Facebook page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='Downloads/chromedriver') # this will open a new page of Google Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://facebook.com\") #manually logged into FB and navigated to appropriate FB page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use selenium to scroll through the page. We adjust the scroll time appropriately, taking into account post length and page popularity, for each particular page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < 80: \n",
    "    SCROLL_PAUSE_TIME = 0.5\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            i+=1\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the file to its appropriate HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"AP.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Bloomberg.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Wonkette.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CNN.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DailyCaller.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WashTimes.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use BeautifulSoup and some modified helper functions to extract information from each appropriate page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup= BeautifulSoup(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostText(content): \n",
    "    \"\"\"A helper function to extrac the text of each post.\n",
    "    It's possible that class names might change over time.\n",
    "    \"\"\"\n",
    "    post_el = content.find(\"div\", \n",
    "                           class_=\"_5pbx userContent _3576\")\n",
    "    if post_el: \n",
    "        return post_el.text\n",
    "    else: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLikesComments(article):\n",
    "    \"\"\"\n",
    "    A helper file to extract the statistics of engagement for each post.\n",
    "    \"\"\"\n",
    "    likes = 0 \n",
    "    comments = 0\n",
    "    shares = 0\n",
    "    \n",
    "    # gather likes \n",
    "    likes_el = article.find(\"span\", class_=\"_81hb\")\n",
    "    if likes_el: \n",
    "        likes = likes_el.text \n",
    "        \n",
    "    # gather comments\n",
    "    comments_el = article.find(\"a\", class_=\"_3hg- _42ft\")\n",
    "    if comments_el:\n",
    "        comments = comments_el.text\n",
    "            \n",
    "    return [likes, comments.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the information from each post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we extract the same information (likes and comments) from each post on Fox News."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = []\n",
    "for article in soup.find_all(\"div\", class_=\"_5pcr userContentWrapper\"):\n",
    "    try:\n",
    "        post_text = getPostText(article)\n",
    "        likes, comments = getLikesComments(article)\n",
    "        post_data.append((post_text, likes, comments))\n",
    "    except AttributeError: \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save our results as a data frame so it can be used later in exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bloomberg_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wonkette_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "DailyCaller_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "WashTimes_df = pd.DataFrame(post_data, columns=[\"text\", \"likes\", \"comments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the cell below to view each appropriate data frame to ensure the post count is ~200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Given the president’s refusal to participate ...</td>\n",
       "      <td>51</td>\n",
       "      <td>[31, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Americans got a dose of politics and debate as...</td>\n",
       "      <td>0</td>\n",
       "      <td>[4, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Just as PETA sent humane bug catchers to cand...</td>\n",
       "      <td>42</td>\n",
       "      <td>[14, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The sheriff’s department stuck an undercover o...</td>\n",
       "      <td>298</td>\n",
       "      <td>[58, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The debate reportedly cut out for nearly three...</td>\n",
       "      <td>403</td>\n",
       "      <td>[107, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>Tulsa Athletic announced it will no longer pla...</td>\n",
       "      <td>231</td>\n",
       "      <td>[103, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>President Trump on Wednesday said top Obama-er...</td>\n",
       "      <td>1.5K</td>\n",
       "      <td>[301, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>In an appearance on CNN, NASCAR’s only black d...</td>\n",
       "      <td>391</td>\n",
       "      <td>[294, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>A 24-year-old real estate investment CEO has w...</td>\n",
       "      <td>243</td>\n",
       "      <td>[60, Comments]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>“None of us have ever been told to slow down o...</td>\n",
       "      <td>585</td>\n",
       "      <td>[239, Comments]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>672 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text likes         comments\n",
       "0    “Given the president’s refusal to participate ...    51   [31, Comments]\n",
       "1    Americans got a dose of politics and debate as...     0    [4, Comments]\n",
       "2    “Just as PETA sent humane bug catchers to cand...    42   [14, Comments]\n",
       "3    The sheriff’s department stuck an undercover o...   298   [58, Comments]\n",
       "4    The debate reportedly cut out for nearly three...   403  [107, Comments]\n",
       "..                                                 ...   ...              ...\n",
       "667  Tulsa Athletic announced it will no longer pla...   231  [103, Comments]\n",
       "668  President Trump on Wednesday said top Obama-er...  1.5K  [301, Comments]\n",
       "669  In an appearance on CNN, NASCAR’s only black d...   391  [294, Comments]\n",
       "670  A 24-year-old real estate investment CEO has w...   243   [60, Comments]\n",
       "671  “None of us have ever been told to slow down o...   585  [239, Comments]\n",
       "\n",
       "[672 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WashTimes_df # view data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results into a JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save our results as a JSON file to be used later in exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('AP-posts.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('Bloomberg-posts.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('Wonkette-posts.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('CNN-posts.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('DailyCaller-posts.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(post_data, open('WashTimes-posts.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
