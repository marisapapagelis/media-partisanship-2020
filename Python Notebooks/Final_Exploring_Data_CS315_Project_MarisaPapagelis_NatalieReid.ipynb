{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 315 Project Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marisa Papagelis and Natalie Reid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore data for our project. At the most basic level, we are scraping Facebook pages for post content (text, likes, and comments). Additional information will be added to this section as our project progresses.\n",
    "\n",
    "We will compliment this notebook with Collecting_Data_CS315_Project_MarisaPapagelis_NatalieReid, a notebook for our data collection. All data we explore will be pulled from that notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section A: Getting to Know the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section A, we will subset collect a subset of our data and run various data exploration and data summarizing representations on it. The main goal of this section is to prepare for the final data exploaration and analysis by figuring out how much data we need and how we would like to analyze it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Sample Exploration: Measuring Frequency of Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data exploration, we randomly selected 175 Facebook posts from each news source page in our data set (300 posts total). We read through the text in these posts and recorded how many posts our chosen keywords appeared in to get an idea of whether or not our data collection methods and keywords were feasible.\n",
    "\n",
    "To do this, we are going to create dictionaries of our keywords for each page, and we will loop through the post texts of our 175 posts for each page recording the number of posts each keyword appears in. This will help us determine whether whether or not our data collection methods and keywords were feasible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load our collected data for all three news sources and assign appropriate names to each JSON file.\n",
    "\n",
    "**In order to replicate our process on a personal computer, the file path for *all* JSON loads may need to be altered. For easiest access, move the 'PM6_MarisaPapagelis_NatalieReid' folder, containing all of our work, to your desktop, and the given file paths should work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/marisapapagelis/Desktop/PM6_MarisaPapagelis_NatalieReid/Python Notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os. getcwd() \n",
    "#current working directory should say '/Users/**your user**/Desktop/PM6_Marisa_Papagelis_NatalieReid/Python Notebooks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../') #back out of 'Python Notebooks' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/reuters-posts.json','r') as infile:\n",
    "    reuters = json.load(infile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/MSNBC-posts.json','r') as infile:\n",
    "    MSNBC = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/FoxNews-posts.json','r') as infile:\n",
    "    foxNews = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we subset our data to the first 175 posts on each news source page for our sample exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = reuters[0:174]\n",
    "MSNBC = MSNBC[0:174]\n",
    "foxNews = foxNews[0:174]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to iterate through the 175 posts on each news source Facebook page, we will use dictionaries. Below, we will create a dictionary for each news page. We will set all of the values to 0 to begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0}\n",
    "MSNBCDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0}\n",
    "foxNewsDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect data, for each news source page we will loop through the text of each post for the predetermined keywords using the dictionaries to keep count. The hasNoText function checks to see if a given post has no text in the body, which gets stored in our dictionary as 'NaN'. The post!=post notation returns true if it has no text, false if otherwise. This function is then incorporated into the countKeywords function because we only want to examine a post for keywords if it actually contains any text. If not, we skip that post and move on to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any posts that do not have text\n",
    "def hasNoText(post):\n",
    "    return post != post\n",
    "\n",
    "# count the frequency of keywords from a site's dictionary of posts\n",
    "def countKeywords(site, siteDict):\n",
    "    for post in site:\n",
    "        for key in siteDict:\n",
    "            if hasNoText(post[0]) == False:\n",
    "                if key in post[0].lower():\n",
    "                    siteDict[key] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run our countKeywords function for each news page we previously scraped. Posts without any text are ignored, in order to avoid errors with the .lower() function. This is a great example of the 'language processing'/'text cleaning' portion of our project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countKeywords(reuters, reutersDict)\n",
    "countKeywords(MSNBC, MSNBCDict)\n",
    "countKeywords(foxNews, foxNewsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we record results from our dictionaries. An example dictionary result for Reuters is printed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'economy': 6, 'healthcare': 0, 'supreme court': 3, 'coronavirus': 32, 'crime': 0, 'foreign policy': 0, 'guns': 0, 'race': 5, 'immigration': 0, 'inequality': 0, 'climate change': 1, 'abortion': 0}\n",
      "{'economy': 1, 'healthcare': 0, 'supreme court': 9, 'coronavirus': 32, 'crime': 0, 'foreign policy': 0, 'guns': 0, 'race': 5, 'immigration': 0, 'inequality': 0, 'climate change': 0, 'abortion': 0}\n",
      "{'economy': 1, 'healthcare': 0, 'supreme court': 11, 'coronavirus': 67, 'crime': 0, 'foreign policy': 0, 'guns': 0, 'race': 1, 'immigration': 0, 'inequality': 0, 'climate change': 0, 'abortion': 0}\n"
     ]
    }
   ],
   "source": [
    "print(reutersDict)\n",
    "print(MSNBCDict)\n",
    "print(foxNewsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding this sample exploration, we think it would be helpful to add more keywords to our dictionary. We will continue our exploration with this in the next part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Follow Up Sample Exploration: Adding Relevant Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing our initial data exploration, we also decided to add five more keywords that we saw in the headlines often: “police”, “government”, “covid”, “president”, and “congress.” We wanted to add these so that our search reflects as many relevant issues as possible. \n",
    "\n",
    "We have to remember to set our original dictionaries back to zero before running the code on them again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0}\n",
    "MSNBCDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0}\n",
    "foxNewsDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we our new keywords to each dictionary. We also set them to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersDict['police'] = 0\n",
    "reutersDict['government'] = 0\n",
    "reutersDict['covid'] = 0\n",
    "reutersDict['president'] = 0\n",
    "reutersDict['congress'] = 0\n",
    "\n",
    "MSNBCDict['police'] = 0\n",
    "MSNBCDict['government'] = 0\n",
    "MSNBCDict['covid'] = 0\n",
    "MSNBCDict['president'] = 0\n",
    "MSNBCDict['congress'] = 0\n",
    "\n",
    "foxNewsDict['police'] = 0\n",
    "foxNewsDict['government'] = 0\n",
    "foxNewsDict['covid'] = 0\n",
    "foxNewsDict['president'] = 0\n",
    "foxNewsDict['congress'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to rerun our code on the updated dictionaries to count the new keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "countKeywords(reuters, reutersDict)\n",
    "countKeywords(MSNBC, MSNBCDict)\n",
    "countKeywords(foxNews, foxNewsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we record results from our dictionaries. An example dictionary result for Reuters is printed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'economy': 6, 'healthcare': 0, 'supreme court': 3, 'coronavirus': 32, 'crime': 0, 'foreign policy': 0, 'guns': 0, 'race': 5, 'immigration': 0, 'inequality': 0, 'climate change': 1, 'abortion': 0, 'police': 3, 'government': 11, 'covid': 43, 'president': 30, 'congress': 1}\n",
      "{'economy': 1, 'healthcare': 0, 'supreme court': 9, 'coronavirus': 32, 'crime': 0, 'foreign policy': 0, 'guns': 0, 'race': 5, 'immigration': 0, 'inequality': 0, 'climate change': 0, 'abortion': 0, 'police': 0, 'government': 0, 'covid': 30, 'president': 84, 'congress': 1}\n",
      "{'economy': 1, 'healthcare': 0, 'supreme court': 11, 'coronavirus': 67, 'crime': 0, 'foreign policy': 0, 'guns': 0, 'race': 1, 'immigration': 0, 'inequality': 0, 'climate change': 0, 'abortion': 0, 'police': 5, 'government': 2, 'covid': 46, 'president': 119, 'congress': 1}\n"
     ]
    }
   ],
   "source": [
    "print(reutersDict)\n",
    "print(MSNBCDict)\n",
    "print(foxNewsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding our keyword frequency exploration, we decided it would be ideal to have 800 posts per news source in our final sample (2400 total). This would ensure that we have enough data to achieve meaningful results and conclusions. In our final sample, we will perform some data analysis techniques on this frequency data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III:  Applying a Data Analysis Technique on our Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will apply a data analysis technique that we encountered during our Literature Review. We will use this application to evaluate whether or not this data analysis technique would be feasible to use on our final sample.\n",
    "\n",
    "A paper that resonated with us during our Literature Review was titled “Social Media News Communities: Gatekeeping, Coverage, and Statement Bias” published by Diego Saez-Trumper, Carlos Castillo, and Mounia Lalmas. These researchers collected a preselected list of news websites chosen from Alexa’s top 100 websites. Over a two week period, they checked each source every 30 minutes for new articles. Next, they aggregated the articles into stories which discussed a common topic or event by measuring the cosine similarity of pairs of articles using TF.IDF weighting.\n",
    "\n",
    "We chose apply this technique by calculating the TF-IDF scores using the NLTK libary and the .TextCollection() API which provides TF, IDF, and TF-IDF abstractions for us to use. We merged this idea with a technique we looked at from Professor Eni Mustafaraj's notebook titled \"Day 11 - Mining Text Files-checkpoint\" that we explored during lecture. We used our predetermined keywords to query text data using TF-IDF. This provides for us a score and a ranking of each of the text posts based on certain keywords. We experimented with different keywords as well as various combinations of keywords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function QueryTextData that takes in a string of keyword(s) as query terms and parses through the terms within the text of each post, ranking each post in order of its similarity to the keyword(s). Finally, the function prints out the resulting texts and scores in a ranked order. The function is designed so the ranking only includes posts in which the text contained at least one of the keyword(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QueryTextData(keywords):\n",
    "    \n",
    "    data = reuters # data from reuters Facebook page\n",
    "    \n",
    "    activities = [] # create an empty list \n",
    "    for post in reuters: # iterate through data and pull out post text\n",
    "        activity = post[0].lower().split() # split words within posts into lists\n",
    "        activities.append(activity) # append each list to empty list \n",
    "            \n",
    "    tc = nltk.TextCollection(activities) # TextCollection provides tf, idf, and tf_idf abstractions\n",
    "    \n",
    "    relevant_activities = [] # create a new list for relevant posts (posts containing keywords)\n",
    "\n",
    "    for idx in range(len(activities)):\n",
    "        score = 0\n",
    "        for term in [t.lower() for t in keywords]:\n",
    "            score += tc.tf_idf(term, activities[idx]) # iterate through terms in each post and calculate score\n",
    "        if score > 0: # only add to relevant list if score > 0 (keyword is in post)\n",
    "            relevant_activities.append({'score': score, 'title': data[idx][0]})\n",
    "    \n",
    "    # sort by score and display results\n",
    "    relevant_activities = sorted(relevant_activities,\n",
    "                             key=lambda p: p['score'], reverse=True)\n",
    "    \n",
    "    for activity in relevant_activities:\n",
    "        print('Title: {0}'.format(activity['title']))\n",
    "        print('Score: {0}'.format(activity['score']))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final piece of our data analysis technique, we will test our function on some randomly selected keywords and combinations of keywords in order to see relevance and rankings.\n",
    "\n",
    "The functions were commented out to save space, but they can be uncommented to view results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QueryTextData('coronavirus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QueryTextData('race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QueryTextData('climate change')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluding our exploration, we decided that ranking our keywords does not provide as much insight as we hoped it would. We will take concept from our exploration to use with our final sample, but we will not use the ranking format as we had hoped to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Building and Testing a Classifier on our Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will create a classification system that can predict whether a given text post comes from either the Reuters (center/moderate), MSNBC (left-wing), or Fox News (right-wing) Facebook page. The code for this section was based off of the Day 14 Sentiment Analysis notebook, specifically section 2. We start by creating our test and training data sets, which are subsets of the text posts we scraped from the three facebook pages. Both of these data sets are large arrays of tuples, where the tuple first contains the text post as a string, and then another string with the name of the news page where it originated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two empty lists to hold the training and testing data\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# subset the scraped reuters data and add it to the correct list\n",
    "reutersTrain = reuters[0:149]\n",
    "reutersTest = reuters[150:174]\n",
    "\n",
    "for post in reutersTrain:\n",
    "    train.append((post[0], \"Reuters\"))\n",
    "    \n",
    "for post in reutersTest:\n",
    "    test.append((post[0], \"Reuters\"))\n",
    "\n",
    "# subset the scraped MSNBC data and add it to the correct list\n",
    "msnbcTrain = MSNBC[0:149]\n",
    "msnbcTest = MSNBC[150:174]\n",
    "\n",
    "for post in msnbcTrain:\n",
    "    train.append((post[0], \"MSNBC\"))\n",
    "\n",
    "for post in msnbcTest:\n",
    "    test.append((post[0], \"MSNBC\"))\n",
    "    \n",
    "# subset the scraped Fox News data and add it to the correct list\n",
    "foxNewsTrain = foxNews[0:149]\n",
    "foxNewsTest = foxNews[150:174]\n",
    "\n",
    "for post in foxNewsTrain:\n",
    "    train.append((post[0], \"Fox News\"))\n",
    "    \n",
    "for post in foxNewsTest:    \n",
    "    test.append((post[0], \"Fox News\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and testing data sets can be viewed in their entirety by uncommenting and running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the Naive Bayes Classifier module, which will help us to train our classifier and use it for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8055555555555556"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, our initial accuracy rating is 80.5%, indicating that our machine can predict the source of a given Facebook news text post correctly 80.5% of the time. Below is the entire output of running our testing data set through the classifier, and it includes the predicted and actual sources next to the corresponding text post. As we can see, the classifier frequently chooses the correct source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We commented the section below for simplicity because it is quite lengthy. Uncomment the cell to view the results including the prediciton, the truth, and the sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Predict | True | Sentence\")\n",
    "# for sentence in test:\n",
    "#     print(cl.classify(sentence[0]) + \" | \"  + sentence[1] + \" | \" + sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were satisfied with how this classifier functioned, as well as the accuracy, so we decided to move forward with using it as our model in our final data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B: Final Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section B, we have our final data set which consists of 1000 posts from each news source we are observing (Reuters, MSNBC, and Fox News). Since the news is always changing, to ensure time as a control, to the best of our ability, **all data was scraped from Reuters, MSNBC, and FoxNews Facebook pages on October 6, 2020 between 11am and 3pm EDT.** \n",
    "\n",
    "Our final dataset will consist of 2400 posts (800 from each of Reuters, MSNBC, and FoxNews). We will evaluate keyword frequency by news source, evaluate average post length by news source, and perform quantitative and qualitative analysis on the results, then we will test and train a classifier on our dataset. Our classifier will be able to predict whether a post is from a right-wing, left-wing, or neutral news source. We will test our classifier on additional news sources and track the accuracy of our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Measuring Keyword Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Keyword Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part I of our final data analysis, we will measure the keyword frequency of our predetermined keywords in our final data set. We will use dictionaries as well as the same process that we used in our sample exploration to do this. Refer back to Section A for in depth documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load our collected data for all three news sources and assign appropriate names to each JSON file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to replicate our process on a personal computer, the file path for *all* JSON loads may need to be altered. For easiest access, move the 'PM6_MarisaPapagelis_NatalieReid' folder, containing all of our work, to your desktop, and the given file paths should work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/reuters-posts.json','r') as infile:\n",
    "    reuters = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/MSNBC-posts.json','r') as infile:\n",
    "    MSNBC = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/FoxNews-posts.json','r') as infile:\n",
    "    foxNews = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subset our data to choose the first 800 posts from each news source to use in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters = reuters[0:799]\n",
    "MSNBC = MSNBC[0:799]\n",
    "foxNews = foxNews[0:799]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create dictionaries with all of our predetermined keywords. Each keyword is set to 0 to prepare for counting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}\n",
    "MSNBCDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}\n",
    "foxNewsDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to collect data, for each news source page we will loop through the text of each post for the predetermined keywords using the dictionaries to keep count. The hasNoText function checks to see if a given post has no text in the body, which gets stored in our dictionary as 'NaN'. The post!=post notation returns true if it has no text, false if otherwise. This function is then incorporated into the countKeywords function because we only want to examine a post for keywords if it actually contains any text. If not, we skip that post and move on to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any posts that do not have text\n",
    "def hasNoText(post):\n",
    "    return post != post\n",
    "\n",
    "# count the frequency of keywords from a site's dictionary of posts\n",
    "def countKeywords(site, siteDict):\n",
    "    for post in site:\n",
    "        for key in siteDict:\n",
    "            if hasNoText(post[0]) == False:\n",
    "                if key in post[0].lower():\n",
    "                    siteDict[key] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the countKeywords function on our three news sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "countKeywords(reuters, reutersDict)\n",
    "countKeywords(MSNBC, MSNBCDict)\n",
    "countKeywords(foxNews, foxNewsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we view our dictionaries to view keyword frequency, make observations, and perform analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'economy': 21, 'healthcare': 4, 'supreme court': 19, 'coronavirus': 161, 'crime': 4, 'foreign policy': 0, 'guns': 2, 'race': 14, 'immigration': 0, 'inequality': 0, 'climate change': 7, 'abortion': 0, 'police': 35, 'government': 56, 'covid': 144, 'president': 96, 'congress': 7}\n",
      "{'economy': 6, 'healthcare': 1, 'supreme court': 45, 'coronavirus': 119, 'crime': 0, 'foreign policy': 1, 'guns': 0, 'race': 13, 'immigration': 0, 'inequality': 2, 'climate change': 3, 'abortion': 0, 'police': 19, 'government': 5, 'covid': 114, 'president': 333, 'congress': 5}\n",
      "{'economy': 6, 'healthcare': 1, 'supreme court': 78, 'coronavirus': 115, 'crime': 0, 'foreign policy': 2, 'guns': 0, 'race': 6, 'immigration': 1, 'inequality': 0, 'climate change': 0, 'abortion': 0, 'police': 42, 'government': 7, 'covid': 66, 'president': 426, 'congress': 11}\n"
     ]
    }
   ],
   "source": [
    "print(reutersDict)\n",
    "print(MSNBCDict)\n",
    "print(foxNewsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of our frequency analysis, in order to see which topics are most relevant to each news source, we will rank the keywords from largest to smallest number of posts each keyword appears in. We sort the dictionaries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersDictSorted = sorted(reutersDict.items(), key=lambda x: x[1], reverse=True)\n",
    "MSNBCDictSorted = sorted(MSNBCDict.items(), key=lambda x: x[1], reverse=True)\n",
    "FoxNewsDictSorted = sorted(foxNewsDict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to format and print the sorted results to be easily interpretable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatSort(newsSource):\n",
    "    for i in newsSource:\n",
    "        print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the results, we call the formatSort function on each sorted dictionary. Reuters is left uncommented as an example. Uncomment the other two functions in order to see full results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters Dict\n",
      "coronavirus 161\n",
      "covid 144\n",
      "president 96\n",
      "government 56\n",
      "police 35\n",
      "economy 21\n",
      "supreme court 19\n",
      "race 14\n",
      "climate change 7\n",
      "congress 7\n",
      "healthcare 4\n",
      "crime 4\n",
      "guns 2\n",
      "foreign policy 0\n",
      "immigration 0\n",
      "inequality 0\n",
      "abortion 0\n"
     ]
    }
   ],
   "source": [
    "print('Reuters Dict')\n",
    "formatSort(reutersDictSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('MSNBC Dict')\n",
    "# formatSort(MSNBCDictSorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Fox News Dict')\n",
    "# formatSort(FoxNewsDictSorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Calculating Average Post Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to further explore our data, we calculated the average number of words in each post for each of our three news sources. \n",
    "\n",
    "First we created empty lists and looped through the text of the posts in all 800 posts in each news source, counting the number of words in each post, and added these values to the empty list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageReuters = []\n",
    "for string in reuters:\n",
    "        averageReuters.append(len(string[0].split()))\n",
    "        \n",
    "averageMSNBC = []\n",
    "for string in MSNBC:\n",
    "        averageMSNBC.append(len(string[0].split()))\n",
    "        \n",
    "averagefoxNews = []\n",
    "for string in foxNews:\n",
    "        averagefoxNews.append(len(string[0].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a function to take the average numbers in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the function on our three news sources in order to find the average number of words in each news source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.50688360450563"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Average(averageReuters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.414267834793492"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Average(averageMSNBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.46683354192741"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Average(averagefoxNews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we will build a classification system to predict whether a given test post comes from either the Reuters (center/moderate), MSNBC (left-wing), or Fox News (right-wing) Facebook Page. This time we will use our larger final data set, which consists of 2400 scraped posts in total. We will split the 800 posts from each page into two subsets, consisting of 700 posts for the training data set and 100 posts for the testing data set. Since we have much more data this time, we are expecting to see our accuracy score improve overall. We also changed the class labels to the more generic \"Center\"/\"Left-wing\"/\"Right-wing\" in preparation for our partisanship exploration later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two empty lists to hold the training and testing data\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "# subset the scraped reuters data and add it to the correct list\n",
    "reutersTrain = reuters[0:699]\n",
    "reutersTest = reuters[700:800]\n",
    "\n",
    "for post in reutersTrain:\n",
    "    train.append((post[0], \"Center\"))\n",
    "    \n",
    "for post in reutersTest:\n",
    "    test.append((post[0], \"Center\"))\n",
    "\n",
    "# subset the scraped MSNBC data and add it to the correct list\n",
    "msnbcTrain = MSNBC[0:699]\n",
    "msnbcTest = MSNBC[700:800]\n",
    "\n",
    "for post in msnbcTrain:\n",
    "    train.append((post[0], \"Left-wing\"))\n",
    "\n",
    "for post in msnbcTest:\n",
    "    test.append((post[0], \"Left-wing\"))\n",
    "    \n",
    "# subset the scraped Fox News data and add it to the correct list\n",
    "foxNewsTrain = foxNews[0:699]\n",
    "foxNewsTest = foxNews[700:800]\n",
    "\n",
    "for post in foxNewsTrain:\n",
    "    train.append((post[0], \"Right-wing\"))\n",
    "    \n",
    "for post in foxNewsTest:    \n",
    "    test.append((post[0], \"Right-wing\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final testing and training data sets can be viewed by uncommenting and running the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test the accuracy of our classification system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.797979797979798"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our model is 80%. This means that our classification system is now able to correctly decide which Facebook page a given post came from about 80% of the time. This is much higher than pure chance, indicating that our system is working really well. Now we'll take a look at what our system predicts for each post in the testing data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We commented the section below for simplicity because it is quite lengthy. Uncomment the cell to view the results including the prediciton, the truth, and the sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Predict | True | Sentence\")\n",
    "# for sentence in test:\n",
    "#     print(cl.classify(sentence[0]) + \" | \"  + sentence[1] + \" | \" + sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that our system might be better at accurately predicting the news page for some networks over others. To investigate this, we decided to create a confusion matrix from our testing data to further understand our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matricies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subsection, we import sklearn.metrics in order to use a function to build our confusion matrix to determine where our classifier might be getting confused. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a list for the prediction of each sentence and a list for the truth of each sentence. We looped through the sentences in our test cohort and add the predictions and truths to the appropriate lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for sentence in test:\n",
    "    y_true.append(sentence[1])\n",
    "    y_pred.append(cl.classify(sentence[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we ran the confusion_matrix function on our data and created a confusion matrix to investigate our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72,  3, 24],\n",
       "       [ 5, 75, 19],\n",
       "       [ 4,  5, 90]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, y_pred, labels=[\"Center\", \"Left-wing\", \"Right-wing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create a visualization of our confusion matrix for presentational purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:68: FutureWarning: Pass labels=['Center', 'Left-wing', 'Right-wing'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[72  3 24]\n",
      " [ 5 75 19]\n",
      " [ 4  5 90]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels = [\"Center\", \"Left-wing\", \"Right-wing\"]\n",
    "cm = confusion_matrix(y_true, y_pred, labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion Matrix')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Percentages of Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an attempt to understand why the classifier works so well on our right-wing news source, we will compare the percentage of **keywords** in the test set for each news source that was seen in the training set of that source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to set the dictionaries back to zero to count the training datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}\n",
    "MSNBCDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}\n",
    "foxNewsDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the countKeywords function on the training sets and view the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "countKeywords(reutersTrain, reutersDict)\n",
    "countKeywords(msnbcTrain, MSNBCDict)\n",
    "countKeywords(foxNewsTrain, foxNewsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'economy': 19, 'healthcare': 2, 'supreme court': 18, 'coronavirus': 134, 'crime': 3, 'foreign policy': 0, 'guns': 2, 'race': 14, 'immigration': 0, 'inequality': 0, 'climate change': 6, 'abortion': 0, 'police': 29, 'government': 51, 'covid': 129, 'president': 93, 'congress': 4}\n",
      "{'economy': 6, 'healthcare': 1, 'supreme court': 41, 'coronavirus': 112, 'crime': 0, 'foreign policy': 1, 'guns': 0, 'race': 13, 'immigration': 0, 'inequality': 1, 'climate change': 3, 'abortion': 0, 'police': 6, 'government': 3, 'covid': 108, 'president': 315, 'congress': 3}\n",
      "{'economy': 6, 'healthcare': 1, 'supreme court': 78, 'coronavirus': 109, 'crime': 0, 'foreign policy': 2, 'guns': 0, 'race': 6, 'immigration': 1, 'inequality': 0, 'climate change': 0, 'abortion': 0, 'police': 27, 'government': 6, 'covid': 63, 'president': 407, 'congress': 10}\n"
     ]
    }
   ],
   "source": [
    "print(reutersDict)\n",
    "print(MSNBCDict)\n",
    "print(foxNewsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to set the dictionaries back to zero to count the testing datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}\n",
    "MSNBCDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}\n",
    "foxNewsDict = {'economy' : 0,'healthcare' : 0, 'supreme court' : 0, 'coronavirus' : 0, 'crime' : 0, 'foreign policy' : 0, 'guns' : 0, 'race' : 0, 'immigration' : 0, 'inequality' : 0, 'climate change' : 0, 'abortion' : 0, 'police': 0, 'government': 0, 'covid': 0, 'president': 0, 'congress': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the countKeywords function on the testing sets and view the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "countKeywords(reutersTest, reutersDict)\n",
    "countKeywords(msnbcTest, MSNBCDict)\n",
    "countKeywords(foxNewsTest, foxNewsDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'economy': 2, 'healthcare': 2, 'supreme court': 1, 'coronavirus': 27, 'crime': 1, 'foreign policy': 0, 'guns': 0, 'race': 0, 'immigration': 0, 'inequality': 0, 'climate change': 1, 'abortion': 0, 'police': 6, 'government': 5, 'covid': 15, 'president': 3, 'congress': 3}\n",
      "{'economy': 0, 'healthcare': 0, 'supreme court': 4, 'coronavirus': 7, 'crime': 0, 'foreign policy': 0, 'guns': 0, 'race': 0, 'immigration': 0, 'inequality': 1, 'climate change': 0, 'abortion': 0, 'police': 13, 'government': 2, 'covid': 6, 'president': 17, 'congress': 2}\n",
      "{'economy': 0, 'healthcare': 0, 'supreme court': 0, 'coronavirus': 6, 'crime': 0, 'foreign policy': 0, 'guns': 0, 'race': 0, 'immigration': 0, 'inequality': 0, 'climate change': 0, 'abortion': 0, 'police': 15, 'government': 1, 'covid': 3, 'president': 19, 'congress': 1}\n"
     ]
    }
   ],
   "source": [
    "print(reutersDict)\n",
    "print(MSNBCDict)\n",
    "print(foxNewsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exported all six of the dictionaries above to an Excel sheet for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After analysis, we decided to not use any of this information in our project. Since the classifier uses *all* words and not just our selected keywords, we decided it would be more effective to measure percantages of *all* words, which we did below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Percentages of *All Words*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to further analyze the accuracy of the classifier, we will look at the percentages of all words in the training data set vs. the testing data set. We use this exploration to answer the following question: \n",
    "\n",
    "### **What percentage of the words in the test data was seen in the training data?**\n",
    "\n",
    "First, we will compare between all testing and all traiing data, and then we will split the data into the three appropriate news sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through the tuples to grab only the post texts, and not the partisanship, from each tuple, and add them to a new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = []\n",
    "for tup in train:\n",
    "    trainSentences.append(tup[0])\n",
    "\n",
    "testSentences = []\n",
    "for tup in test: \n",
    "    testSentences.append(tup[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a training and a testing count vectorizer, and we fit the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2097x7916 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 57451 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizerTrain = CountVectorizer()\n",
    "X_train = vectorizerTrain.fit_transform(trainSentences)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<297x2347 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7083 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizerTest = CountVectorizer()\n",
    "X_test = vectorizerTest.fit_transform(testSentences)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use .get_feature_names() to parse through the vectorizer and create a list of all of the unique words within a list of sentences. We use this extension in addition to a counter to loop through the test and train unique features and figure out how many unique words in the test data set are in the training data set. Lastly, we divide the number of unique words in the training and testing set by the number of the words in the testing set, to get the percentage we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664\n",
      "2347\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for word in vectorizerTest.get_feature_names(): \n",
    "    if word in vectorizerTrain.get_feature_names():\n",
    "        counter += 1\n",
    "print(counter)\n",
    "print(len(vectorizerTest.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1664/2347) * 100 = 70.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this code again on the testing and training sets from Reuters, MSNBC, and Fox News. \n",
    "We loop through the tuples to grab only the post texts, and not the partisanship, from each tuple, and add them to a new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersTrainSentences = []\n",
    "for tup in reutersTrain:\n",
    "    reutersTrainSentences.append(tup[0])\n",
    "\n",
    "reutersTestSentences = []\n",
    "for tup in test: \n",
    "    reutersTestSentences.append(tup[0])\n",
    "    \n",
    "msnbcTrainSentences = []\n",
    "for tup in msnbcTrain:\n",
    "    msnbcTrainSentences.append(tup[0])\n",
    "\n",
    "msnbcTestSentences = []\n",
    "for tup in test: \n",
    "    msnbcTestSentences.append(tup[0])\n",
    "\n",
    "foxNewsTrainSentences = []\n",
    "for tup in foxNewsTrain:\n",
    "    foxNewsTrainSentences.append(tup[0])\n",
    "\n",
    "foxNewsTestSentences = []\n",
    "for tup in test: \n",
    "    foxNewsTestSentences.append(tup[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a training and a testing count vectorizer, and we fit the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reutersVectorizerTrain = CountVectorizer()\n",
    "X_train = reutersVectorizerTrain.fit_transform(reutersTrainSentences)\n",
    "\n",
    "reutersVectorizerTest = CountVectorizer()\n",
    "X_test = reutersVectorizerTest.fit_transform(reutersTestSentences)\n",
    "\n",
    "msnbcVectorizerTrain = CountVectorizer()\n",
    "X_train = msnbcVectorizerTrain.fit_transform(msnbcTrainSentences)\n",
    "\n",
    "msnbcVectorizerTest = CountVectorizer()\n",
    "X_test = msnbcVectorizerTest.fit_transform(msnbcTestSentences)\n",
    "\n",
    "foxNewsVectorizerTrain = CountVectorizer()\n",
    "X_train = foxNewsVectorizerTrain.fit_transform(foxNewsTrainSentences)\n",
    "\n",
    "foxNewsVectorizerTest = CountVectorizer()\n",
    "X_test = foxNewsVectorizerTest.fit_transform(foxNewsTestSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use .get_feature_names() to parse through the vectorizer and create a list of all of the unique words within a list of sentences. We use this extension in addition to a counter to loop through the test and train unique features and figure out how many unique words in the test data set are in the training data set. Lastly, we divide the number of unique words in the training and testing set by the number of the words in the testing set, to get the percentage we are looking for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1274\n",
      "2347\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for word in reutersVectorizerTest.get_feature_names(): \n",
    "    if word in reutersVectorizerTrain.get_feature_names():\n",
    "        counter += 1\n",
    "print(counter)\n",
    "print(len(reutersVectorizerTest.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1274/2347) * 100 = 54.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144\n",
      "2347\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for word in msnbcVectorizerTest.get_feature_names(): \n",
    "    if word in msnbcVectorizerTrain.get_feature_names():\n",
    "        counter += 1\n",
    "print(counter)\n",
    "print(len(msnbcVectorizerTest.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1144/2347) * 100 = 48.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fox News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164\n",
      "2347\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for word in foxNewsVectorizerTest.get_feature_names(): \n",
    "    if word in foxNewsVectorizerTrain.get_feature_names():\n",
    "        counter += 1\n",
    "print(counter)\n",
    "print(len(foxNewsVectorizerTest.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1164/2347) * 100 = 49.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will record these results to analyze in our results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Classifier on Additional News Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how accurate our classifier would be on news sources that are not the three we trained it on, we chose 6 more news sources to run our classifier on. We chose two historically neutral sources, AP and Bloomberg, two left-wing, Wonkette, CNN, and two right-wing, InfoWars, The Washington Times. *Both InfoWars and Wonkette are historically less reliable and contain propagated information, so we decided to include these sources to analyze what happens.* We made these selections using an Interactive Bias Media Chart (https://www.adfontesmedia.com/interactive-media-bias-chart-2/) that we referred to when choosing our intial three sources as well as during our Literature Review\n",
    "\n",
    "We decided to scrape 200 of the most recent posts from all six of our news sources and run them through the classifier to test to accuracy. Since our classifier was trained to work for any post, time does not need to be controlled in this step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load in the appropriate json files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to replicate our process on a personal computer, the file path for *all* JSON loads may need to be altered. For easiest access, move the 'PM6_MarisaPapagelis_NatalieReid' folder, containing all of our work, to your desktop, and the given file paths should work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/AP-posts.json','r') as infile:\n",
    "    AP = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/Bloomberg-posts.json','r') as infile:\n",
    "    Bloomberg = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/Wonkette-posts.json','r') as infile:\n",
    "    Wonkette = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/CNN-posts.json','r') as infile:\n",
    "    CNN = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/DailyCaller-posts.json','r') as infile:\n",
    "    DailyCaller = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('JSON files/WashTimes-posts.json','r') as infile:\n",
    "    WashTimes = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subset the files to hold only the first 200 posts for each selected news source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP = AP[0:199]\n",
    "Bloomberg = Bloomberg[0:199]\n",
    "Wonkette = Wonkette[0:199]\n",
    "CNN = CNN[0:199]\n",
    "DailyCaller = DailyCaller[0:199]\n",
    "WashTimes = WashTimes[0:199]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the testing data set from the scraped posts of the other six news pages. Again, we'll use the generic labels of \"left\", \"center\", and \"right\" because we're specifically looking at partisanship. The first 200 posts from each of these JSON files are already subsetted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = []\n",
    "\n",
    "for post in AP:\n",
    "    test2.append((post[0], \"Center\"))\n",
    "\n",
    "for post in Bloomberg:\n",
    "    test2.append((post[0], \"Center\"))\n",
    "    \n",
    "for post in Wonkette:\n",
    "    test2.append((post[0], \"Left-wing\"))\n",
    "\n",
    "for post in CNN: \n",
    "    test2.append((post[0], \"Left-wing\"))\n",
    "    \n",
    "for post in DailyCaller:\n",
    "    test2.append((post[0], \"Right-wing\"))\n",
    "    \n",
    "for post in WashTimes:\n",
    "    test2.append((post[0], \"Right-wing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire testing data set can be viewed by uncommenting and running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll test our new classifier on the additional scraped posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4380234505862647"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We commented the section below for simplicity because it is quite lengthy. Uncomment the cell to view the results including the prediciton, the truth, and the sentence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Predict | True | Sentence\")\n",
    "# for sentence in test2:\n",
    "#     print(cl.classify(sentence[0]) + \" | \"  + sentence[1] + \" | \" + sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy for this classifier is much lower at just under 42%. While this is still slightly above chance (33%), it doesn't inspire much confidence that our classifier is actually picking up on partisanship instead of linguistic intricacies. To look into this issue further, we will test each of the new test data sets on the trained classifier, in order to see which ones are more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data sets for each individual news source\n",
    "\n",
    "APTest = []\n",
    "for post in AP:\n",
    "    APTest.append((post[0], \"Center\"))\n",
    "    \n",
    "BloombergTest = []\n",
    "for post in Bloomberg:\n",
    "    BloombergTest.append((post[0], \"Center\"))\n",
    "    \n",
    "WonketteTest = []\n",
    "for post in Wonkette:\n",
    "    WonketteTest.append((post[0], \"Left-wing\"))\n",
    "    \n",
    "CNNTest = []\n",
    "for post in CNN:\n",
    "    CNNTest.append((post[0], \"Left-wing\"))\n",
    "    \n",
    "DailyCallerTest = []\n",
    "for post in DailyCaller:\n",
    "    DailyCallerTest.append((post[0], \"Right-wing\"))\n",
    "    \n",
    "WashTimesTest = []\n",
    "for post in WashTimes:\n",
    "    WashTimesTest.append((post[0], \"Right-wing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are each of the new data sets being run individually in the classifier. They are broken up in this way because each takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3316582914572864"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(APTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32663316582914576"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(BloombergTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6180904522613065"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(WonketteTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41708542713567837"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(CNNTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5477386934673367"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(DailyCallerTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3869346733668342"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.accuracy(WashTimesTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are really interesting. The accuracy scores have a lot of variation, and it looks as though the two news pages for which our classifier could accurately predict partisanship were Wonkette and The Daily Caller. These two pages were chosen because they both are classified as hyper-partisan and as sharing misleading or false information according to the Ad Fontes media bias chart mentioned earlier. Overall, the classifier did a better job of accurately labeling left-wing news sources, suggesting that these pages have a more prominent linguistic style that is specific to them. None of these accuracy scores are very high  or inspire a lot of confindence (except for those for Wonkette and The Daily Caller), which could indicate that our classifier is using attributes such as post length and word frequency in order to make its predictions, instead of semantic partisanship."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
